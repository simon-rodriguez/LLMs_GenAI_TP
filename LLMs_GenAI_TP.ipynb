{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "\n",
    "- Se usan embeddings con los documentos y se cargan en una base de datos vectorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APIs\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#SPECS\n",
    "PINECONE_CLOUD = os.environ.get('PINECONE_CLOUD')\n",
    "PINECONE_REGION = os.environ.get('PINECONE_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los documentos en pdf\n",
    "loader = PyPDFLoader(\"./data/CV_Simon.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Crear un splitter para dividir en chunks de 500 caracteres\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Tamaño máximo de cada chunk\n",
    "    chunk_overlap=50  # Superposición entre chunks\n",
    ")\n",
    "\n",
    "# Dividir los documentos en chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Imprimir los chunks generados\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "chunk_embeddings = [embedding_model.encode(chunk.page_content).tolist() for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(chunk_embeddings[0])\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Index Specs\n",
    "pc_cloud = PINECONE_CLOUD or \"aws\"\n",
    "pc_region = PINECONE_REGION or \"us-east-1\"\n",
    "\n",
    "\n",
    "# Se abre la conexión con Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"cv-index\"\n",
    "\n",
    "# Se crea el índice si no existe\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=pc_cloud, \n",
    "            region=pc_region\n",
    "        ) \n",
    "    ) \n",
    "\n",
    "# Wait for the index to be ready\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for idx, emb in enumerate(chunk_embeddings):\n",
    "    records.append({\n",
    "        \"id\": str(idx),\n",
    "        \"values\": emb,\n",
    "        \"metadata\": {'text': chunks[idx].page_content }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se conecta al índice\n",
    "pinecone_index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "\n",
    "# Se insertan los chunks en el índice\n",
    "pinecone_index.upsert(\n",
    "    vectors=records,\n",
    "    namespace=\"default\"\n",
    ")\n",
    "\n",
    "# For batches\n",
    "# from tqdm.auto import tqdm\n",
    "# for batch in tqdm(records.iter_documents(batch_size=500), total=160):\n",
    "#     index.upsert(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'default': {'vector_count': 13}},\n",
       " 'total_vector_count': 13}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se conecta al índice\n",
    "pinecone_index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "\n",
    "# view index stats\n",
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta a la base de datos\n",
    "def get_closest_answer(user_query):\n",
    "    # Obtener el embedding de la consulta\n",
    "    query_embedding = embedding_model.encode([user_query]).tolist() # Necesario convertirlo a una lista\n",
    "\n",
    "    # Buscar el vector más cercano usando Pinecone\n",
    "    result = pinecone_index.query(\n",
    "        namespace=\"default\",\n",
    "        vector=query_embedding, \n",
    "        top_k=1,\n",
    "        include_metadata=True,\n",
    "        include_values=False,\n",
    "        )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the retrieved information to the LLM system prompt\n",
    "def get_system_prompt(user_query):\n",
    "    result = get_closest_answer(user_query)\n",
    "    matched_info = ' '.join(item['metadata']['text'] for item in result['matches'])\n",
    "    context = f\"Information: {matched_info}\"\n",
    "    sys_prompt = f\"\"\"\n",
    "    Instructions:\n",
    "    - Be helpful and answer questions concisely. If you don't know the answer, say 'I don't know'\n",
    "    - Utilize the context provided for accurate and specific information.\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "    return sys_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    # GROQ_API_KEY is the default and can be omitted if in ENV variables\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "def sent_query_to_groq(sys_prompt, user_query):\n",
    "    # Define the query\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_query,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "    # Get the response\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simon estudió en la Universidad Nacional Experimental “Antonio José de Sucre” (VEN).\n"
     ]
    }
   ],
   "source": [
    "user_query = \"En que universidad estudió simon?\"\n",
    "#user_query = \"What is Simon's experience?\"\n",
    "#user_query = \"What are Simon's skills?\"\n",
    "#user_query = \"What are Simon's interests?\"\n",
    "\n",
    "sys_prompt = get_system_prompt(user_query)\n",
    "\n",
    "sent_query_to_groq(sys_prompt, user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipo:\n",
    "- Andres Malvestti\n",
    "- Cristian Davico\n",
    "- Simon Rodriguez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2\n",
    "\n",
    "- Se utilizan agentes para responder especificamente sobre cada uno de los documentos.\n",
    "- Si no se presenta nombre, entonces traer uno por defecto.\n",
    "- PLUS: Si se consulta por más de un CV, traer el contexto de cada uno de forma acorde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos:\n",
    "\n",
    "1. Esquematizar el diagrama de flujo que debe tener la aplicación.\n",
    "2. Definir el \"conditional Edge\", tomador de la decisión. Consejo: utilizaar la librería **re** y su método **match**.\n",
    "3. Implementar cada uno de los pasos y compilar el diagrama de flujo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se preparan y cargan los CVs en índices separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from groq import Groq\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from transformers import AutoModel\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APIs\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para cargar un PDF, dividirlo en chunks y obtener los embeddings\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_chunks(documents):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,  # Tamaño máximo de cada chunk\n",
    "        chunk_overlap=50  # Superposición entre chunks\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def print_chunks(chunks):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")\n",
    "\n",
    "def get_embeddings(chunks):\n",
    "    embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "    chunk_embeddings = [embedding_model.encode(chunk.page_content).tolist() for chunk in chunks]\n",
    "    dim = len(chunk_embeddings[0])\n",
    "    return chunk_embeddings, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "simon_cv = \"./data/CV_Simon.pdf\"\n",
    "jess_cv = \"./data/CV_Jess.pdf\"\n",
    "\n",
    "simon_index = \"simon-index\"\n",
    "jess_index = \"jess-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simon\n",
    "documents_s = load_pdf(simon_cv)\n",
    "chunks_s = split_chunks(documents_s)\n",
    "print_chunks(chunks_s)\n",
    "chunk_embeddings_s, dim_s = get_embeddings(chunks_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jess\n",
    "documents_j = load_pdf(jess_cv)\n",
    "chunks_j = split_chunks(documents_j)\n",
    "print_chunks(chunks_j)\n",
    "chunk_embeddings_j, dim_j = get_embeddings(chunks_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Specs\n",
    "pc_cloud = \"aws\"\n",
    "pc_region = \"us-east-1\"\n",
    "\n",
    "# Funcion para crear un índice en Pinecone\n",
    "def create_index(index_name, dim):\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    # Se crea el índice si no existe\n",
    "    if not pc.has_index(index_name):\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dim,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=pc_cloud, \n",
    "                region=pc_region\n",
    "            ) \n",
    "        ) \n",
    "\n",
    "    # Wait for the index to be ready\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "def insert_chunks(index_name, chunk_embeddings, chunks):\n",
    "    records = []\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    for idx, emb in enumerate(chunk_embeddings):\n",
    "        records.append({\n",
    "            \"id\": str(idx),\n",
    "            \"values\": emb,\n",
    "            \"metadata\": {'text': chunks[idx].page_content }\n",
    "        })\n",
    "\n",
    "    # Se conecta al índice\n",
    "    pinecone_index = pc.Index(index_name)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Se insertan los chunks en el índice\n",
    "    pinecone_index.upsert(\n",
    "        vectors=records,\n",
    "        namespace=\"default\"\n",
    "    )\n",
    "\n",
    "def index_describe(index_name):\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    pinecone_index = pc.Index(index_name)\n",
    "    time.sleep(1)\n",
    "    return pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean los indices para cada CV\n",
    "create_index(simon_index, dim_s)\n",
    "create_index(jess_index, dim_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agregan los chunks a los índices\n",
    "insert_chunks(simon_index, chunk_embeddings_s, chunks_s)\n",
    "insert_chunks(jess_index, chunk_embeddings_j, chunks_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'default': {'vector_count': 35}},\n",
       " 'total_vector_count': 35}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se describen los índices\n",
    "index_describe(simon_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'default': {'vector_count': 28}},\n",
       " 'total_vector_count': 28}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_describe(jess_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se definen los agentes y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una clase Agente para interactuar con el sistema, esta es la estructura del agente\n",
    "class Agent:\n",
    "    def __init__(self, sys_prompt=\"\"):\n",
    "        self.system = sys_prompt\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": sys_prompt})\n",
    "\n",
    "    def __call__(self, user_query):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        completion = client.chat.completions.create(\n",
    "                        model=\"llama3-8b-8192\", \n",
    "                        messages=self.messages)\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una consulta a la base de datos\n",
    "def get_closest_answer(user_query, index_name):\n",
    "    # Obtener el embedding de la consulta\n",
    "    embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "    query_embedding = embedding_model.encode([user_query]).tolist() # Necesario convertirlo a una lista\n",
    "\n",
    "\n",
    "    # Se abre la conexión con Pinecone\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    # Se conecta al índice\n",
    "    pinecone_index = pc.Index(index_name)\n",
    "\n",
    "    # Buscar el vector más cercano usando Pinecone\n",
    "    result = pinecone_index.query(\n",
    "        namespace=\"default\",\n",
    "        vector=query_embedding, \n",
    "        top_k=1,\n",
    "        include_metadata=True,\n",
    "        include_values=False,\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Agrega la información al prompt del sistema para dar el contexto\n",
    "def get_system_prompt(user_query, index_name):\n",
    "    result = get_closest_answer(user_query, index_name)\n",
    "    matched_info = ' '.join(item['metadata']['text'] for item in result['matches'])\n",
    "    context = f\"Information: {matched_info}\"\n",
    "    print(index_name)\n",
    "    sys_prompt = f\"\"\"\n",
    "    Instructions:\n",
    "    - Be helpful and answer questions concisely. If you don't know the answer, say 'I don't know'\n",
    "    - Utilize the context provided for accurate and specific information.\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "    return sys_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agente_simon(user_query):\n",
    "    sys_prompt = get_system_prompt(user_query, \"simon-index\")\n",
    "    agent = Agent(sys_prompt)\n",
    "    result = agent(user_query)\n",
    "    return result\n",
    "\n",
    "def agente_jess(user_query):\n",
    "    sys_prompt = get_system_prompt(user_query, \"jess-index\")\n",
    "    agent = Agent(sys_prompt)\n",
    "    result = agent(user_query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_re = re.compile(r'^ACT: call \"(\\w+)\" for query \"(.*)\"$', re.MULTILINE)  # expresión regular para capturar secuencias de texto\n",
    "\n",
    "acciones_disponibles = {\n",
    "    \"cv_simon\": agente_simon,\n",
    "    \"cv_jessica\": agente_jess,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PROMPT = f\"\"\"\n",
    "Instructions: You run in a cycle of THINK, ACT, WAIT, and RESULT. At the end of the cycle, you give an ANSWER.\n",
    "\n",
    "THINK: Read the user query and think about an action to answer it.\n",
    "ACT: Reply with calling an Action to perform any of the actions available to you. This should be formatted as ACT: call \"action\" for query \"user query\".\n",
    "WAIT: Wait until you receive the next prompt with the result of your action.\n",
    "RESULT: you will recieve the result of your action in the next prompt.\n",
    "ANSWER: finally, answer the user query with the result you received.\n",
    "\n",
    "Available actions:\n",
    "- cv_simon: use it if you want to ask about Simon's CV\n",
    "- cv_jessica: use it if you want to ask about Jessica's CV\n",
    "\n",
    "DO NOT FORGET TO WAIT FOR THE RESULT BEFORE ANSWERING THE USER QUERY. DO NOT SKIP ANY STEP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(user_query, max_turns=2):\n",
    "    i = 0\n",
    "    bot = Agent(INITIAL_PROMPT)\n",
    "    next_prompt = user_query\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        print(\"Turno\", i)\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        acciones = [\n",
    "            action_re.search(a)\n",
    "            for a in result.split('\\n') \n",
    "            if action_re.search(a)\n",
    "        ]\n",
    "        if acciones:\n",
    "            # There is an action to run\n",
    "            accion, accion_input = acciones[0].groups()\n",
    "            print(\"Acción detectada:\", accion)\n",
    "            if accion not in acciones_disponibles:\n",
    "                raise Exception(\"Acción desconocida: {}: {}\".format(accion, accion_input))\n",
    "            print(\" -- corriendo {} {}\".format(accion, accion_input))\n",
    "            observacion = acciones_disponibles[accion](accion_input)\n",
    "            print(\"RESULT:\", observacion)\n",
    "            next_prompt = \"RESULT: {}\".format(observacion)\n",
    "        else:\n",
    "            print(\"No hay acciones detectadas\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turno 1\n",
      "THINK: Ah, Jessica's CV, let me think for a moment...\n",
      "\n",
      "ACT: call \"cv_jessica\" for query \"What did Jessica study?\"\n",
      "\n",
      "WAIT: Please respond with the result of my action.\n",
      "\n",
      "(Please respond with the result, and I'll proceed with answering the user query)\n",
      "Acción detectada: cv_jessica\n",
      " -- corriendo cv_jessica What did Jessica study?\n",
      "jess-index\n",
      "RESULT: I don't have information about Jessica's education or academic background.\n",
      "Turno 2\n",
      "THINK: Okay, it seems I don't have enough information about Jessica's education. Let me think what to do next...\n",
      "\n",
      "ACT: Sorry, I don't have the information to answer that query. I'll let the user know that Jessica's education or academic background is unknown.\n",
      "\n",
      "ANSWER: Sorry, I don't have information about Jessica's education or academic background.\n",
      "No hay acciones detectadas\n"
     ]
    }
   ],
   "source": [
    "question = \"What did Jessica study?\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turno 1\n",
      "THINK: Hmm, the user is asking about Simon's background...\n",
      "\n",
      "ACT: call \"cv_simon\" for query \"What did Simon study?\"\n",
      "\n",
      "WAIT: (Waiting for the result...)\n",
      "Acción detectada: cv_simon\n",
      " -- corriendo cv_simon What did Simon study?\n",
      "simon-index\n",
      "RESULT: Simon Rodriguez A. is a Mechanical Engineer.\n",
      "Turno 2\n",
      "THINK: Ah, I've got the result! Simon is a Mechanical Engineer...\n",
      "\n",
      "ANSWER: Simon studied Mechanical Engineering.\n",
      "No hay acciones detectadas\n"
     ]
    }
   ],
   "source": [
    "question = \"What did Simon study?\"\n",
    "query(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
